{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b89b5a93-827c-4088-b6d6-d1545720d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e14e56c-9b61-4932-b142-ba263cd4bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = 'paint'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'apple2orange'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' # 'build' # \n",
    "\n",
    "TRAIN_DATA_FOLDER = './data/apple2orange/trainA/*.jpg'\n",
    "\n",
    "TEST_DATA_FOLDER = './data/apple2orange/testA/*.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1b7ab1-6162-4b17-b4da-44a992f41f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "IMAGE_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d9dbb0-739e-492d-b94f-d5c4fed1843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([   \n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, filenames, transform):\n",
    "        self.filenames = glob(filenames)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = Image.open(self.filenames[idx])\n",
    "        img2_path = self.filenames[idx]\n",
    "        img2_path = img2_path.replace('trainA', 'trainB')\n",
    "\n",
    "        img2 = Image.open(self.filenames[idx])\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            \n",
    "        return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6b9adf-6ef8-43eb-969f-b5bc7b0d7be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleDataset(TRAIN_DATA_FOLDER, transform)\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE,drop_last = True, shuffle=True)\n",
    "\n",
    "test_dataset = SimpleDataset(TEST_DATA_FOLDER, transform)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, drop_last = True, shuffle=False)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de23d6d9-b61f-4814-80ce-a1f335f57b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, gen_n_filters):\n",
    "        super().__init__()\n",
    "        self.gen_n_filters = gen_n_filters\n",
    "\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(3, self.gen_n_filters, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(self.gen_n_filters, self.gen_n_filters*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*2, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(self.gen_n_filters*2, self.gen_n_filters*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*4, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(self.gen_n_filters*4, self.gen_n_filters*8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*8, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(self.gen_n_filters*8, self.gen_n_filters*4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*4, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(self.gen_n_filters*4*2, self.gen_n_filters*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*2, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(self.gen_n_filters*2*2, self.gen_n_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters, affine=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(self.gen_n_filters*2, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        u1 = self.up1(d4)\n",
    "        u2 = self.up2(torch.cat((u1, d3), dim=1))\n",
    "        u3 = self.up3(torch.cat((u2, d2), dim=1))\n",
    "        output_img = self.up4(torch.cat((u3, d1), dim=1))\n",
    "        return output_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff3bf0-3dae-4e73-af97-cfd40b8353d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d625c9-8864-42fc-b82d-97a37526d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, gen_n_filters):\n",
    "        super().__init__()\n",
    "        self.gen_n_filters = gen_n_filters\n",
    "        \n",
    "        \n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(3, self.gen_n_filters, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(0.2)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(self.gen_n_filters, self.gen_n_filters*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*2, affine=True),\n",
    "            nn.ReLU(0.2)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(self.gen_n_filters*2, self.gen_n_filters*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*4, affine=True),\n",
    "            nn.ReLU(0.2)\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(self.gen_n_filters*4, self.gen_n_filters*8, kernel_size=4, stride=1, padding=2),\n",
    "            nn.InstanceNorm2d(self.gen_n_filters*8, affine=True),\n",
    "            nn.ReLU(0.2),\n",
    "            nn.Conv2d(self.gen_n_filters*8, 1, 4, 1, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.down1(x)        \n",
    "\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.down4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d25fdad-a647-4539-82a9-7e88163a1060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 128, 128]), torch.Size([1, 1, 16, 16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GeneratorUNet(32)(torch.randn(128, 3, 128, 128)).shape, Discriminator(32)(torch.randn(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2dddff3-5450-4f06-8062-dab4d3fde22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_AB = GeneratorUNet(32).to(device)\n",
    "g_BA = GeneratorUNet(32).to(device)\n",
    "d_A = Discriminator(32).to(device)\n",
    "d_B = Discriminator(32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a2fc0b7-a72b-40ce-9004-fc808a8778fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ggh5454\\.conda\\envs\\pytorch_env1\\lib\\site-packages\\torch\\optim\\adam.py:90: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    }
   ],
   "source": [
    "d_A_optimizer = optim.Adam(d_A.parameters(), lr=0.5, betas=(0.5, 0.999))\n",
    "d_B_optimizer = optim.Adam(d_B.parameters(), lr=0.5, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "import itertools\n",
    "g_optimizer = optim.Adam(itertools.chain(g_AB.parameters(), g_AB.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d079cbe4-bca6-425a-9cd1-44bf7c1bad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_save_folder = os.path.join(RUN_FOLDER, 'images')\n",
    "model_save_path =  os.path.join(RUN_FOLDER, 'weights/cycle_gan_apple2orange.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "735b3330-d7db-4407-ba04-2f77f83a363c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run/paint/0001_apple2orange\\\\images'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_save_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d10e9b4-8283-4a4c-9ada-b2b3e5ab4a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c1e90-ce7d-4747-94b5-4ba8fb60db61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [D loss: 1058.816263] [G loss: 2037.919722, adv: 2025.750528, recon: 1.103909, id: 1.130214]\n",
      "[Epoch 1/200] [D loss: 0.234620] [G loss: 11.760035, adv: 0.671353, recon: 1.005937, id: 1.029312]\n"
     ]
    }
   ],
   "source": [
    "lambda_validation = 1\n",
    "lambda_reconstr = 10\n",
    "lambda_id = 1\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "real_tensor = torch.ones(BATCH_SIZE, 1, 16, 16).to(device)\n",
    "fake_tensor = torch.zeros(BATCH_SIZE, 1, 16, 16).to(device)\n",
    "epochs = 200\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "validation_losses = []\n",
    "reconstr_losses = []\n",
    "id_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_validation_loss = 0.0\n",
    "    epoch_reconstr_loss = 0.0\n",
    "    epoch_id_loss = 0.0\n",
    "    epoch_D_loss = 0.0\n",
    "    epoch_G_loss = 0.0\n",
    "    for (real_A, real_B) in train_dataloader:\n",
    "        # Move data to device\n",
    "\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        fake_B = g_AB(real_A)\n",
    "        fake_A = g_BA(real_B)\n",
    "\n",
    "\n",
    "        d_A_optimizer.zero_grad()\n",
    "        d_A_real_output = d_A(real_A)\n",
    "        d_A_real_loss = criterion(d_A_real_output, real_tensor)\n",
    "        valid_A = d_A(fake_A)\n",
    "        d_A_fake_loss = criterion(valid_A, fake_tensor)\n",
    "        d_A_loss = 0.5 * ( d_A_real_loss+ d_A_fake_loss)\n",
    "        d_A_loss.backward()\n",
    "        d_A_optimizer.step()\n",
    "\n",
    "        d_B_optimizer.zero_grad()\n",
    "        d_B_real_output = d_B(real_B)\n",
    "        d_B_real_loss = criterion(d_B_real_output, real_tensor)\n",
    "        valid_B = d_B(fake_B)\n",
    "        d_B_fake_loss = criterion(valid_B, fake_tensor)\n",
    "        d_B_loss = 0.5 * ( d_B_real_loss+ d_B_fake_loss)\n",
    "        d_B_loss.backward()\n",
    "        d_B_optimizer.step()\n",
    "        d_loss = 0.5 * (d_A_loss + d_B_loss)\n",
    "\n",
    "\n",
    "        # Generator\n",
    "        for p in d_A.parameters():\n",
    "            p.requires_grad = False\n",
    "        for p in d_B.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "\n",
    "        fake_B = g_AB(real_A)\n",
    "        fake_A = g_BA(real_B)\n",
    "        reconstr_A = g_BA(fake_B)\n",
    "        reconstr_B = g_AB(fake_A)\n",
    "        id_A = g_BA(real_A)\n",
    "        id_B = g_AB(real_B)\n",
    "        valid_A = d_A(fake_A)\n",
    "        valid_B = d_B(fake_B)\n",
    "\n",
    "        validation_loss = criterion(real_tensor, valid_A) +\\\n",
    "                          criterion(real_tensor, valid_B)\n",
    "        reconstr_loss = F.l1_loss(real_A, reconstr_A) +\\\n",
    "                        F.l1_loss(real_B, reconstr_B)\n",
    "        id_loss = F.l1_loss(real_A, id_A) +\\\n",
    "                  F.l1_loss(real_B, id_B)\n",
    "        g_loss = lambda_validation * validation_loss + lambda_reconstr * reconstr_loss + lambda_id * id_loss\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        for p in d_A.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in d_B.parameters():\n",
    "            p.requires_grad = True\n",
    "        epoch_D_loss += d_loss.item() * real_A.size(0)\n",
    "        epoch_G_loss += g_loss.item() * real_A.size(0)\n",
    "        epoch_validation_loss += validation_loss.item() * real_A.size(0)\n",
    "        epoch_reconstr_loss += reconstr_loss.item() * real_A.size(0)\n",
    "        epoch_id_loss += id_loss.item() * real_A.size(0)\n",
    "    epoch_D_loss /= len(train_dataloader)\n",
    "    epoch_G_loss /= len(train_dataloader)\n",
    "    epoch_validation_loss /= len(train_dataloader)\n",
    "    epoch_reconstr_loss /= len(train_dataloader)\n",
    "    epoch_id_loss /= len(train_dataloader)\n",
    "    \n",
    "    D_losses.append(epoch_D_loss)\n",
    "    G_losses.append(epoch_G_loss)\n",
    "    validation_losses.append(epoch_validation_loss)\n",
    "    reconstr_losses.append(epoch_reconstr_loss)\n",
    "    id_losses.append(epoch_id_loss)\n",
    "    print(\"[Epoch %d/%d] [D loss: %f] [G loss: %05f, adv: %05f, recon: %05f, id: %05f]\"\\\n",
    "        % (epoch, epochs,\n",
    "           epoch_D_loss, \n",
    "           epoch_G_loss,\n",
    "           epoch_validation_loss,\n",
    "           epoch_reconstr_loss,\n",
    "           epoch_id_loss))\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        with torch.no_grad():\n",
    "            test_A_images, test_B_images = next(iter(test_dataloader))\n",
    "            test_A_image = test_A_images[0].to(device).reshape(1, 3, 128, 128)\n",
    "            test_B_image = test_B_images[0].to(device).reshape(1, 3, 128, 128)\n",
    "            fake_B = g_AB(test_A_image)\n",
    "            fake_A = g_BA(test_B_image)\n",
    "            reconstr_A = g_BA(fake_B)\n",
    "            reconstr_B = g_AB(fake_A)\n",
    "            id_A = g_BA(test_A_image)\n",
    "            id_B = g_AB(test_B_image)\n",
    "\n",
    "        result_imgs = torch.cat([test_A_image, fake_B, reconstr_A, id_A,\n",
    "                                 test_B_image, fake_A, reconstr_B, id_B], dim=0).detach().cpu()\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(result_imgs,\n",
    "                                                 nrow=4,\n",
    "                                                 padding=2,\n",
    "                                                 normalize=True), (1, 2, 0)))\n",
    "        plt.savefig(os.path.join(image_save_folder, f'result_img_e_{epoch + 1}.png'))\n",
    "\n",
    "        torch.save({'d_A': d_A.state_dict(),\n",
    "                    'd_B': d_B.state_dict(),\n",
    "                    'g_AB': g_AB.state_dict(),\n",
    "                    'g_BA': g_BA.state_dict()},\n",
    "                   model_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5e3c5-0390-4ce5-b165-8a916508012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot([x for x in D_losses], color='black', linewidth=1)\n",
    "plt.plot([x for x in validation_losses], color='green', linewidth=1)\n",
    "plt.plot([x for x in reconstr_losses], color='blue', linewidth=1)\n",
    "plt.plot([x for x in id_losses], color='red', linewidth=1)\n",
    "\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "plt.savefig(os.path.join(image_save_folder, 'loss_graph.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de623e-fb87-433c-910e-6cc914a80692",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "for inputs_A, inputs_B in test_dataloader:\n",
    "    inputs_A = inputs_A.to(device)\n",
    "    inputs_B = inputs_B.to(device)\n",
    "\n",
    "    fake_B = g_AB(inputs_A)\n",
    "    fake_A = g_BA(inputs_B)\n",
    "    reconstr_A = g_BA(fake_B)\n",
    "    reconstr_B = g_AB(fake_A)\n",
    "    id_A = g_BA(inputs_A)\n",
    "    id_B = g_AB(inputs_B)\n",
    "    valid_A = d_A(fake_A)\n",
    "    valid_B = d_B(fake_B)\n",
    "\n",
    "    result_imgs = torch.cat([inputs_A, fake_B, reconstr_A, id_A,\n",
    "                            inputs_B, fake_A, reconstr_B, id_B], dim=0).detach().cpu()\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(result_imgs,\n",
    "                                                nrow=4,\n",
    "                                                padding=2,\n",
    "                                                normalize=True), (1, 2, 0)))\n",
    "    plt.savefig(os.path.join(image_save_folder, f'test_input_result_img_{idx}.png'))\n",
    "    idx += 1\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
