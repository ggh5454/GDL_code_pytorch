{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "section = 'vae'\n",
    "run_id = '0001'\n",
    "data_name = 'faces'\n",
    "RUN_FOLDER = 'run/{}/'.format(section)\n",
    "RUN_FOLDER += '_'.join([run_id, data_name])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "\n",
    "\n",
    "DATA_FOLDER = './data/celeb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 32\n",
    "transform = transforms.Compose([   \n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
    "NUM_IMAGES = len(filenames)\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, filenames, transform):\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.filenames[idx])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleDataset(filenames, transform)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE)\n",
    "z_dim = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 128, 128])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.dropout3 = nn.Dropout()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, 2)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.dropout4 = nn.Dropout()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mu = nn.Linear(4096, z_dim)\n",
    "\n",
    "        self.log_var = nn.Linear(4096, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(self.conv1(x), (1,0,1,0))\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.pad(self.conv2(x), (1,0,1,0))\n",
    "        x = self.bn2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.pad(self.conv3(x), (1,0,1,0))\n",
    "        x = self.bn3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.pad(self.conv4(x), (1,0,1,0))\n",
    "        x = self.bn4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        mu, log_var = self.mu(x), self.log_var(x)\n",
    "\n",
    "        def sampling(args):\n",
    "            mu, log_var = args\n",
    "            epsilon = torch.normal(0., 1., size=mu.shape).to(device)\n",
    "            return mu + torch.exp(log_var / 2) * epsilon\n",
    "\n",
    "        x = sampling([mu, log_var])\n",
    "\n",
    "        return x, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.5133,  0.6966,  2.7349, -1.6937,  0.5595, -0.2291,  0.9756,  1.9582,\n",
       "           1.2483,  0.4462,  0.8618,  0.3774,  0.6404, -1.0817, -0.0484, -0.5272,\n",
       "          -1.4039,  1.2009,  1.2682,  2.3018, -2.1301,  0.7920,  1.1097, -0.7670,\n",
       "           0.1408, -1.4177, -0.7196, -0.4259, -0.2756,  2.1120, -1.5853, -0.0338,\n",
       "           0.3483,  1.0878, -0.1384,  0.2455, -0.6010, -0.8431, -2.3416, -1.4999,\n",
       "          -2.1460,  1.4183,  1.0914, -0.8100,  1.1454, -1.5163, -1.3616,  1.0333,\n",
       "           0.1617, -1.2724, -1.2692, -2.3126,  1.2156,  0.0880, -0.5975,  0.1915,\n",
       "          -0.1213, -1.2717, -0.1225, -1.1866,  0.9553,  1.1409,  0.3968, -2.6861,\n",
       "           2.5686, -0.4801, -0.0055,  0.0073,  1.6582,  0.7641, -0.6670, -1.4761,\n",
       "           0.3748, -0.7810,  0.5270, -2.5496,  0.8778,  0.4304, -0.1256, -0.2407,\n",
       "          -0.3008, -4.7570, -0.3675, -0.4609,  0.1293,  1.9294,  0.0860, -0.1600,\n",
       "           1.4306, -0.4745, -0.2443,  0.4694, -0.3394,  0.3838, -0.0125,  1.1362,\n",
       "          -0.9724, -2.0080, -0.1166,  0.1210,  0.9210,  0.2098, -0.2845,  1.2154,\n",
       "          -0.8649, -0.0764, -1.2343, -1.8850,  0.9502,  1.1947, -0.6531, -0.5351,\n",
       "           0.8480, -0.6912,  0.7527, -0.1114,  0.3246, -1.1397, -0.7614,  0.3950,\n",
       "           0.1403,  3.6961,  4.0404,  0.2824,  2.0856,  1.5806,  0.1151,  1.0630,\n",
       "          -0.1604, -0.1479,  0.0197, -0.4847, -1.9560,  2.4544, -0.9333,  1.1006,\n",
       "          -0.2534, -0.5918, -0.2938,  1.9304, -0.5915,  0.4418, -1.3657, -0.9746,\n",
       "          -1.3209, -1.0568, -1.5118, -1.3987, -0.5156, -1.1412, -1.6576,  0.3328,\n",
       "           1.0377, -0.6117,  1.4376,  0.3290, -0.1711,  0.4153,  1.4984,  0.3698,\n",
       "           0.2265,  0.2980,  1.0651,  1.1452,  0.7837,  2.6933, -0.4353, -1.5060,\n",
       "           0.1539, -0.2248,  0.4798, -1.1719, -0.3978, -1.7260, -0.8658, -1.2163,\n",
       "           1.7464, -2.0150,  0.2277, -1.5970, -2.6252, -0.1755, -0.0226, -0.3727,\n",
       "           0.1187, -0.5217,  2.9489, -2.0558,  0.5612,  0.2102,  1.3119,  0.4215,\n",
       "          -0.6952,  2.0511, -0.7169,  1.1413,  2.7858, -0.0482, -1.4827,  2.1831]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[ 9.3553e-02,  3.4840e-02,  1.0103e+00, -1.1876e+00, -1.4970e-01,\n",
       "          -3.5903e-01,  3.2828e-01, -4.3882e-01, -1.5079e-03,  4.1195e-01,\n",
       "           1.6608e-01,  8.9498e-01,  4.6238e-01, -6.5049e-01,  5.6090e-01,\n",
       "           1.1374e-02, -5.2276e-01,  3.6916e-01,  4.8443e-01,  1.6703e-01,\n",
       "          -6.1553e-01, -7.5871e-01,  1.5519e-01, -7.9975e-01, -2.3145e-01,\n",
       "           3.5406e-01,  9.5470e-02,  1.9385e-01,  4.3024e-01,  1.0197e+00,\n",
       "          -8.5709e-01, -7.1822e-02, -2.0854e-01,  7.4498e-01, -1.9169e-01,\n",
       "           1.2431e-01, -7.0244e-02, -7.8435e-02, -1.1082e+00,  5.5584e-01,\n",
       "          -2.5137e-01,  1.0249e+00,  6.8659e-01, -1.4600e-01,  1.5748e+00,\n",
       "          -4.9316e-01, -5.3455e-01, -1.5078e-01,  4.8588e-01, -2.3910e-01,\n",
       "          -5.1388e-01, -6.5519e-01,  2.7738e-01,  3.6432e-01, -7.8340e-01,\n",
       "           2.0776e-01,  7.1750e-01, -4.2814e-01, -1.2676e-01, -5.2725e-01,\n",
       "           7.3727e-01,  5.2033e-02, -1.9479e-01, -5.9639e-01,  1.4245e+00,\n",
       "           3.5278e-01,  2.5528e-01, -8.9829e-01,  1.0686e-01,  6.5627e-01,\n",
       "           6.0360e-01,  7.0664e-01, -4.7309e-01,  1.2180e-02,  7.9745e-01,\n",
       "          -5.3857e-01,  8.2035e-01, -5.8954e-01,  5.4942e-02,  1.2506e-01,\n",
       "           7.0623e-02, -2.1761e+00, -4.5234e-01,  1.6804e-01,  2.2057e-01,\n",
       "           1.8095e-01,  4.9306e-01, -1.0623e-01,  8.5983e-01, -9.5937e-01,\n",
       "           7.3127e-01, -2.3088e-01, -4.8030e-01, -1.3398e-01,  4.2441e-01,\n",
       "          -1.4259e-01,  4.0253e-01,  5.8559e-02, -7.7499e-01,  4.6407e-01,\n",
       "          -2.7132e-01,  1.0501e-01, -4.0277e-02,  7.6227e-01, -3.2189e-01,\n",
       "           1.1989e-01, -3.7331e-01, -2.6936e-03, -3.9160e-02,  1.4557e-01,\n",
       "          -6.9402e-01, -1.9735e-01, -3.2823e-01,  4.0352e-01, -1.5861e-01,\n",
       "           1.5078e-01, -2.2799e-01, -3.6215e-01,  1.3119e-01,  8.4812e-01,\n",
       "           1.2356e+00,  1.5982e+00,  8.4798e-01, -1.9968e-01, -9.9151e-02,\n",
       "           4.3803e-01,  6.4165e-01,  4.7190e-01, -4.9906e-02,  7.3945e-01,\n",
       "           5.1052e-01,  9.1612e-02, -8.1344e-01,  2.9514e-01, -8.1841e-02,\n",
       "           4.9558e-02, -3.6615e-02, -2.8199e-01,  5.0935e-01, -8.9595e-02,\n",
       "           1.2784e-01, -7.9114e-02, -5.3131e-01, -3.6021e-01,  3.3432e-01,\n",
       "           1.0546e-01, -3.6004e-01, -8.9649e-01, -5.7721e-01, -3.2676e-01,\n",
       "          -7.8118e-02,  1.0134e+00,  6.4755e-01, -4.7943e-01,  6.6055e-01,\n",
       "          -1.0099e-01,  9.6014e-01,  4.3936e-01, -3.7854e-02,  2.0292e-01,\n",
       "           1.4505e-01,  3.9466e-01,  1.1612e+00,  6.1474e-02,  9.4358e-01,\n",
       "          -6.4182e-01, -1.1407e+00, -2.4319e-01, -7.8482e-01, -4.3415e-01,\n",
       "          -3.6634e-01, -1.6773e-01, -6.9465e-01, -7.0372e-01, -4.9150e-01,\n",
       "          -1.1686e+00,  1.4973e-01, -9.3193e-01,  4.1751e-02, -8.2500e-01,\n",
       "          -8.0925e-01,  1.3878e-01, -1.3550e+00, -5.3027e-01,  1.3532e-01,\n",
       "           9.0687e-01, -5.5997e-01, -1.4352e+00,  5.7976e-01, -2.5147e-01,\n",
       "           6.3791e-01,  1.2846e+00,  2.5901e-01,  6.8226e-01, -2.0491e-01,\n",
       "           1.1854e+00, -5.1340e-01,  1.5769e+00, -8.9802e-01,  4.2555e-01]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.1755, -0.0817,  0.8236,  0.2356, -0.9972,  0.3527,  0.4668,  0.0860,\n",
       "          -0.7316,  0.1206, -0.0226, -0.4029,  0.1458,  0.0419, -1.2559, -0.8344,\n",
       "          -0.8656,  1.1653,  0.0629,  0.7920,  0.5738, -0.2677,  0.1710,  0.5663,\n",
       "           0.0406,  0.5524,  0.1503, -0.6357, -0.2121,  0.3122, -1.0894,  0.5973,\n",
       "          -0.1003, -0.9123, -0.8292, -0.2330,  0.6199, -0.1044,  0.0484,  0.3443,\n",
       "           0.9042,  0.2383, -0.4553,  0.3705, -0.0712, -0.6604, -0.0363,  0.5404,\n",
       "          -0.7884, -0.1641,  0.2764,  1.4345, -0.3270,  0.1889, -0.7391,  0.4446,\n",
       "          -0.6897, -0.3064, -0.6195, -0.2213,  0.1024,  0.2407,  0.4039,  0.2226,\n",
       "           0.2810, -0.0503, -0.8542, -0.1337, -0.8859,  0.1497, -0.2020,  0.8428,\n",
       "           0.2421, -0.7974, -0.1105, -0.0295, -0.7022, -0.6268,  0.4829,  0.0666,\n",
       "          -0.6831,  0.5200, -1.5940,  0.6789, -1.4059, -0.0887,  0.5154, -0.3975,\n",
       "          -1.1272, -0.7964, -0.5482,  0.1505,  0.4257,  0.2704, -0.5117, -0.6849,\n",
       "           0.6823,  0.4882,  0.7207, -0.1812,  0.8552, -0.0291, -0.2633,  0.5160,\n",
       "          -0.1490,  0.8990, -0.1287,  0.9103, -0.8018, -0.2381, -0.1434, -0.5767,\n",
       "           0.3844,  0.0435, -0.1914,  0.7198,  0.1628, -0.3517,  0.7291,  0.1618,\n",
       "          -0.3047, -0.9374,  0.1651,  0.3765,  0.6157,  0.4999,  0.3292, -0.3997,\n",
       "           0.2737,  0.3589, -0.2606, -0.3804,  0.1197, -0.3415, -1.3102,  0.6974,\n",
       "           0.2278, -0.7449,  0.3004,  0.2172,  1.5916, -0.6619,  0.2319, -0.3595,\n",
       "           0.2807, -0.2045, -0.7568,  0.8951, -0.1598,  0.3348,  0.6846,  1.1349,\n",
       "           0.3328, -0.7663, -0.1225, -0.2521, -0.4287,  0.5666, -0.2912,  0.9248,\n",
       "          -0.8835, -0.6578, -0.6850,  0.4794, -1.1472,  0.6841,  0.1924,  0.6118,\n",
       "           0.6008, -0.1374, -0.2835,  1.0134, -1.2730,  0.8320, -0.1394, -0.2964,\n",
       "           0.1043, -0.0758, -0.5242, -1.2371, -0.3085, -0.7058, -0.4053,  0.7474,\n",
       "           0.4180,  0.3712,  0.6133,  0.2322, -1.1947, -0.2095,  0.2320, -0.9876,\n",
       "           0.3933,  1.0454, -0.5680,  1.2998,  1.3974, -0.3410,  0.4844,  0.6379]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encoder().to(device)(torch.randn((1, 3, 128, 128)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, 64*6*6)\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose2d(64, 64, 3, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(64, 64, 3, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "\n",
    "        self.convT3 = nn.ConvTranspose2d(64, 32, 3, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.dropout3 = nn.Dropout()\n",
    "\n",
    "        self.convT4 = nn.ConvTranspose2d(32, 3, 3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = x.reshape(-1, 64, 6, 6)\n",
    "\n",
    "        x = F.pad(self.convT1(x), (2,1,2,1))      \n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.pad(self.convT2(x), (2,1,2,1))      \n",
    "        x = self.bn2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.pad(self.convT3(x), (2,1,2,1))      \n",
    "        x = self.bn3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.pad(self.convT4(x), (2,1,2,1)) \n",
    "        \n",
    "        return torch.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 128])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decoder().to(device)(torch.randn((1, 200)).to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder().to(device)\n",
    "        self.decoder = Decoder().to(device)\n",
    "    def forward(self, x):\n",
    "        t, mu, log_var = self.encoder(x)\n",
    "        x = self.decoder(t)\n",
    "        return x, t, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout3): Dropout(p=0.5, inplace=False)\n",
       "    (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout4): Dropout(p=0.5, inplace=False)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (mu): Linear(in_features=4096, out_features=200, bias=True)\n",
       "    (log_var): Linear(in_features=4096, out_features=200, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (linear1): Linear(in_features=200, out_features=2304, bias=True)\n",
       "    (convT1): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (convT2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "    (convT3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dropout3): Dropout(p=0.5, inplace=False)\n",
       "    (convT4): ConvTranspose2d(32, 3, kernel_size=(3, 3), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =AutoEncoder().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_LOSS_FACTOR = 100\n",
    "\n",
    "def kl_loss(mu, log_var):\n",
    "    kl_loss =  -0.5 * torch.sum(1 + log_var - torch.square(mu) - torch.exp(log_var))\n",
    "    return kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 72.955\n",
      "[2] loss: 16.845\n",
      "[3] loss: 16.421\n",
      "[4] loss: 16.348\n",
      "[5] loss: 16.306\n",
      "[6] loss: 17.016\n",
      "[7] loss: 16.379\n",
      "[8] loss: 16.407\n",
      "[9] loss: 16.452\n",
      "[10] loss: 16.437\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, inputs in enumerate(dataloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, _, mu, log_var = model(inputs)\n",
    "        loss = criterion(outputs, inputs) * R_LOSS_FACTOR\n",
    "\n",
    "        loss += kl_loss(mu, log_var)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f'[{epoch + 1}] loss: {running_loss / len(dataloader):.3f}')\n",
    "    torch.save(model, RUN_FOLDER + \"/weights/weight.pt\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "726535c0f6f932cb5fb7f4ccf074a920e012d87bcb619fd5c2bde3239c103324"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
