{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0001'\n",
    "DATA_NAME = 'camel'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = np.load(glob('data/camel/*')[0])\n",
    "\n",
    "# train_size = int(len(total) * 0.8)\n",
    "train, test = total[:80000], total[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, npy, transform):\n",
    "        self.npy = npy\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.npy)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.npy[idx]\n",
    "        if self.transform:\n",
    "            img = (img.astype('float32') - 127.5) / 127.5\n",
    "            img = img.reshape(28, 28, 1) \n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([   \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SimpleDataset(train, transform)\n",
    "test_dataset = SimpleDataset(test, transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5, 2)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5, 2)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, 2)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 128, 5, 1)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(2048, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.pad(self.conv1(x), (1,1,1,1))\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.pad(self.conv2(x), (1,1,1,1))\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.pad(self.conv3(x), (2,2,2,2))\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.pad(self.conv4(x), (1,1,1,1))\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4976]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Discriminator()(torch.randn(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, 3136)\n",
    "        self.bn1 = nn.BatchNorm1d(3136, momentum=0.9)\n",
    "        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64, 128, 5, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128, momentum=0.9)\n",
    "        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(128, 64, 5, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64, momentum=0.9)\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose2d(64, 64, 5, 1, 2)\n",
    "        self.bn4 = nn.BatchNorm2d(64, momentum=0.9)\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(64, 1, 5, 1, 2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.linear1(x))\n",
    "        x = F.relu(x)\n",
    "        x = x.reshape(-1, 64, 7, 7)\n",
    "        x = self.upsample1(x)\n",
    "        x = F.pad(self.conv1(x), (2,2,2,2))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.upsample2(x)\n",
    "        x = F.pad(self.conv2(x), (2,2,2,2))\n",
    "        x = F.relu(self.bn3(x))\n",
    "\n",
    "        x = self.convT1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.convT2(x)\n",
    "\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator().to(device)\n",
    "generator =  Generator(100).to(device)\n",
    "d_optimizer = optim.RMSprop(discriminator.parameters(),lr=0.0008)\n",
    "g_optimizer = optim.RMSprop(generator.parameters(), lr=0.0004)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_losses, g_losses = [], []\n",
    "for epoch in range(200):  # loop over the dataset multiple times\n",
    "    g_total, d_total = 0., 0.\n",
    "    for i, inputs in enumerate(train_dataloader, 0):\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        real = torch.ones((BATCH_SIZE, 1)).to(device)\n",
    "        fake = torch.zeros((BATCH_SIZE, 1)).to(device)\n",
    "        generator.eval()\n",
    "        generated_images = generator(torch.randn((BATCH_SIZE, z_dim)).to(device))\n",
    "\n",
    "        d_loss = criterion(discriminator(generated_images), fake)\n",
    "        d_loss += criterion(discriminator(inputs), real)\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.eval()\n",
    "        real = torch.ones((BATCH_SIZE, 1)).to(device)\n",
    "        generated_images = generator(torch.randn((BATCH_SIZE, z_dim)).to(device))\n",
    "\n",
    "        g_loss = criterion(discriminator(generated_images), real)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        d_total += d_loss.item()\n",
    "        g_total += g_loss.item()\n",
    "    \n",
    "    d_losses.append(d_total/len(train_dataloader))\n",
    "    g_losses.append(g_total/len(train_dataloader))\n",
    "\n",
    "    print(f'[{epoch + 1}] d_loss: {d_total / len(train_dataloader):.3f} g_loss: {g_total / len(train_dataloader):.3f}')\n",
    "    torch.save(discriminator, RUN_FOLDER + \"/weights/discriminator.pt\")\n",
    "    torch.save(generator, RUN_FOLDER + \"/weights/generator.pt\")\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ggh5454\\Desktop\\GAN\\GDL_code_pytorch\\04_01_gan_camel_train.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ggh5454/Desktop/GAN/GDL_code_pytorch/04_01_gan_camel_train.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ggh5454/Desktop/GAN/GDL_code_pytorch/04_01_gan_camel_train.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot([x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m d_losses], color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m'\u001b[39m, linewidth\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ggh5454/Desktop/GAN/GDL_code_pytorch/04_01_gan_camel_train.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot([x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m g_losses], color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39morange\u001b[39m\u001b[39m'\u001b[39m, linewidth\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ggh5454/Desktop/GAN/GDL_code_pytorch/04_01_gan_camel_train.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, fontsize\u001b[39m=\u001b[39m\u001b[39m18\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in d_losses], color='black', linewidth=0.25)\n",
    "\n",
    "plt.plot([x for x in g_losses], color='orange', linewidth=0.25)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "726535c0f6f932cb5fb7f4ccf074a920e012d87bcb619fd5c2bde3239c103324"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
