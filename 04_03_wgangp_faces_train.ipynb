{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q-pq-Vpen7Xu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4dChyw0Rn8Ti"
   },
   "outputs": [],
   "source": [
    "# run params\n",
    "SECTION = 'gan'\n",
    "RUN_ID = '0003'\n",
    "DATA_NAME = 'celeb'\n",
    "RUN_FOLDER = 'run/{}/'.format(SECTION)\n",
    "RUN_FOLDER += '_'.join([RUN_ID, DATA_NAME])\n",
    "\n",
    "if not os.path.exists(RUN_FOLDER):\n",
    "    os.mkdir(RUN_FOLDER)\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'viz'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'images'))\n",
    "    os.mkdir(os.path.join(RUN_FOLDER, 'weights'))\n",
    "\n",
    "mode =  'build' #'load' #\n",
    "DATA_FOLDER = './data/celeb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D9oOPIV8gk3J"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0Uo9xlHVgk6W"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([   \n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
    "NUM_IMAGES = len(filenames)\n",
    "\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, filenames, transform):\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.filenames[idx])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IHvqrfCggk8_"
   },
   "outputs": [],
   "source": [
    "dataset = SimpleDataset(filenames, transform)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE,drop_last = True)\n",
    "z_dim = 200\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7a8weXvegk_v"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 5, 2)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 5, 2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(8192, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (2, 2, 2, 2))\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = F.pad(x, (2, 2, 2, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = F.pad(x, (2, 2, 2, 2))\n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = F.pad(x, (2, 2, 2, 2))\n",
    "        x = self.conv4(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zPg5mm-UglB-"
   },
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(z_dim, 8192)\n",
    "        self.bn1 = nn.BatchNorm1d(8192, momentum=0.9)\n",
    "\n",
    "        self.convT1 = nn.ConvTranspose2d(512, 256, 5, 2, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(256, momentum=0.9)\n",
    "\n",
    "        self.convT2 = nn.ConvTranspose2d(256, 128, 5, 2, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(128, momentum=0.9)\n",
    "\n",
    "        self.convT3 = nn.ConvTranspose2d(128, 64, 5, 2, 2)\n",
    "        self.bn4 = nn.BatchNorm2d(64, momentum=0.9)\n",
    "        self.convT4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.linear1(x))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = x.reshape(-1, 512, 4, 4)\n",
    "\n",
    "        x = F.pad(self.convT1(x), (0, 1, 0, 1))\n",
    "        x = self.bn2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = F.pad(self.convT2(x), (0, 1, 0, 1))\n",
    "        x = F.leaky_relu(self.bn3(x))\n",
    "\n",
    "        x = F.pad(self.convT3(x), (0, 1, 0, 1))\n",
    "        x = F.leaky_relu(self.bn4(x))\n",
    "        # x = F.pad(x, (1, 1, 1, 1))\n",
    "        x = self.convT4(x)\n",
    "        return torch.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QenVgJHVglEv"
   },
   "outputs": [],
   "source": [
    "critic = Critic().to(device)\n",
    "generator =  Generator(100).to(device)\n",
    "d_optimizer = optim.Adam(critic.parameters(),lr=0.00005)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PrVTBhL5glK3"
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(real_images, fake_images, critic):\n",
    "    \"\"\" Calculates the gradient penalty.\n",
    "    This loss is calculated on an interpolated image\n",
    "    and added to the discriminator loss.\n",
    "    \"\"\"\n",
    "    epsilon = torch.randn(real_images.shape[0], 1, 1, 1).to(device)\n",
    "    x_hat = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "    t = critic(x_hat)\n",
    "    x_hat = (torch.sum(x_hat.square(),dim=1).sqrt() - 1).square()\n",
    "    return x_hat.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gfgNzdwDglN3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] d_loss: -6839685033875.797 g_loss: -195819906168.818\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gp\n\u001b[0;32m     21\u001b[0m     d_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m     \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     d_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m generator\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch_env1\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch_env1\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_critic = 5\n",
    "d_losses, g_losses = [], []\n",
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    g_total, d_total = 0., 0.\n",
    "    for i, inputs in enumerate(dataloader, 0):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "            generator.eval()\n",
    "            generated_images = generator(torch.randn((BATCH_SIZE, z_dim)).to(device))\n",
    "            \n",
    "            gp = gradient_penalty(inputs, generated_images, critic)\n",
    "            d_loss = - torch.mean(critic(generated_images))\n",
    "            \n",
    "            d_loss += torch.mean(critic(inputs))\n",
    "            \n",
    "            \n",
    "            d_loss += gp\n",
    "\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "        generator.train()\n",
    "        critic.eval()\n",
    "        real = torch.ones((BATCH_SIZE, 1)).to(device)\n",
    "        generated_images = generator(torch.randn((BATCH_SIZE, z_dim)).to(device))\n",
    "\n",
    "        g_loss = - torch.mean(critic(generated_images))\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        d_total += d_loss.item()\n",
    "        g_total += g_loss.item()\n",
    "    \n",
    "    d_losses.append(d_total/len(dataloader))\n",
    "    g_losses.append(g_total/len(dataloader))\n",
    "\n",
    "    print(f'[{epoch + 1}] d_loss: {d_total / len(dataloader):.3f} g_loss: {g_total / len(dataloader):.3f}')\n",
    "    torch.save(critic, RUN_FOLDER + \"/weights/critic.pt\")\n",
    "    torch.save(generator, RUN_FOLDER + \"/weights/generator.pt\")\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P7roXWsrglQv"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot([x for x in d_losses], color='red', linewidth=1)\n",
    "\n",
    "plt.plot([y for y in g_losses], color='orange', linewidth=1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "726535c0f6f932cb5fb7f4ccf074a920e012d87bcb619fd5c2bde3239c103324"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
